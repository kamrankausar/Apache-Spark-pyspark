{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Spark always revolving around these APIs called RDD, Dataframe and DataSet APIs.\n",
    "-> Which is the best API? RDD or Dataframe or DataSet?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Collection of Java or Scala objects that follows Immutability, distributed, fault tolerance properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> RDD API these Scala functions (Transformations & Actions) to compute the data. Its main advantage if you know Scala functions, it’s easy to compute data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> main dis-advantage in RDDs is, it’s using Java serialization by default. Either Java or Scala running JVM only so both using Java Serialization only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Java serialization consume huge amount of resources to serialize data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> avro serialization, it’s internally compress data so that little advantage to improve performance.\n",
    "RDD using Java Serialization, so it’s decrease performance. If you process large amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Kyro serialization little optimize Spark RDD jobs, but you must follow some terms and conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> One more dis-advantage is Jvva serialization is sending both data and it’s structure between nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> If you are processing unstructured data, Rdd highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It’s very powerful mainly focus on performance and to run SQL queries on top of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Simple words a collection of RDDs plus Schema called DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DataFrame, the data is organized into named columns like RDBMS. Means Structure separated, data separated.\n",
    "Spark understands the data schema, so no need to use Java serialization to encode the data, Serialize only data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Spark developer, can easily run SQL queries on top of distributed data, additionally support DSL commands, \n",
    "so Scala programmer also easily run Scala commands. These features not available in RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " DataFrame API is catalyst optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It internally apply logical plans and physical plans, finally based on cost based model, choose the best optimized plan.\n",
    "So It’s internally optimize data compare with RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame also using Java serialization, so like RDDs same dis-advantages available in Data-frame also.\n",
    "Means main advantage optimize performance, and make user friendly, dis-advantage serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " IOT framework called Flink, it’s internally using two powerful APIs called DataSet and DataStream APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " DataSet used to process batch data, DataStream api used to process streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark 2.0 only dataset available, there is no dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between RDD, DataFrame and DataSet is Serialization and Performance.\n",
    "This DataSet api internally using a special serialization called encoder, it’s very powerful than java serialization.  It support Rdd transformations and dataframe DSL commands and allows SQL queries as well.\n",
    "Means if you know rdd and dataframes same steps you can apply in dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
